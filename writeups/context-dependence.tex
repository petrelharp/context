\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{bm}
\usepackage{todonotes}

\newcommand{\peter}[1]{\todo[color=green!40]{Peter: #1}}
\newcommand{\erick}[1]{\todo[color=purple!40]{Erick: #1}}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\calS}{\mathcal{S}}  % set of states
\newcommand{\calT}{\mathcal{T}}  % set of transition triples
\newcommand{\calP}{\mathcal{P}}  % set of selection pairs
\newcommand{\nA}{\mbox{A}}  % nucleotides:
\newcommand{\nC}{\mbox{C}}
\newcommand{\nG}{\mbox{G}}
\newcommand{\nT}{\mbox{T}}
\newcommand{\join}{\oplus}  % matches
\newcommand{\st}{\colon}  % such that
\newcommand{\var}{\mathop{\mbox{var}}}
\newcommand{\cov}{\mathop{\mbox{cov}}}
\newcommand{\given}{\;\mid\;}
\newcommand{\like}{\mathcal L}
\newcommand{\loglike}{\ell}
\newcommand{\alike}{\widetilde{\like}}
\newcommand{\aloglike}{\widetilde{\loglike}}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\bibliographystyle{plainnat}

\begin{document}

\title{Inference using context-dependent models of mutation}
\author{}

\maketitle

\begin{abstract}
\end{abstract}

\section*{Introduction}

It not uncommon to encounter models
consisting of a lattice of sites,
each site taking one of a finite collection of possible states,
and whose stochastic, temporal evolution is Markov and governed by a set of local rules.
For instance, at each site may sit
a nucelotide whose mutation rate depends on physical properties determined by the nearby DNA sequence;
a cell whose infection status depends on the state of its neighbors;
or a particle whose spin is perturbed by external noise to a state whose distribution depends on the local energy configuration.
Practical use of such models often requires
inferring transition rules
based on observations of the system at several time points,
or of several states evolved along a tree from a single starting point.

The general class of models we consider are known in the probability literature as
interacting particle systems \citep{liggett2005ips} with neighborhood structure
-- continuous-time Markov chains on lattice-indexed collections of states
whose transition probabilities are \emph{local}
in the sense that any instantaneous change only affects a small number of nearby sites,
and the rates of such instantaneous changes depend only on the states in some bounded neighborhood of those sites to change.
As finite-state Markov chains,
transition probabilities are in principle simply expressable as a matrix exponential,
but this is impractical because the size of the matrix is equal to the number of possible configurations.

In this paper, we propose a solution to this problem,
showing that the conditional likelihood of local pattern count statistics
can be well-approximated by marginalizing over a finite amount of surrounding context.
Motivated by the problem of inferring context-dependent mutation rates from diverged nucleotide sequences,
we extend the inference framework to observations on trees.
\peter{something about efficient computation and sparse matrices}

The situation is similar to the widely-studied problem of inference based on observation
of a single instance of a Markov random field,
and in many cases reduces to this if we only observe the system at one time point at stationarity.
These are used, for instance, in spatial statistics \citep{besag1972nearestneighbour,gelfand2010handbook}
and image reconstruction \citep{geman1984stochastic,besag1986statistical}.
The conditioning method we consider here
is simliar to the ``coding'' scheme introduced by \citet{besag1974spatial},
that conditions on a set of sites that makes the remaining observations independent (thanks to the Gibbs property);
consistency of such methods has been shown by \citet{comets1992consistency} and reviewed by \citet{larribe2011composite}.
A good review of recent statistical techniques is given by \citet{friel2012bayesian}.


In the context of genomics,
it is well-known that certain short nucleotide sequences
are in many organisms much more, or less, abundant than expected by chance \citep{burge1992underrepresentation},
due to the combined effects of context dependence of the nucleotide mutation process,
selective constraints on the function of the sequence,
and other processes such as biased gene conversion \citep{duret2009biased}.
Indeed, molecular studies have demonstrated that the spectrum of new mutations in humans
is highly context dependent \citep{schaibley2013influence}
and that multinucleotide substitutions are relatively common \citep{schrider2011pervasive,terekhanova2013prevalence,harris2013errorprone}.
(We are concerned with effects that are \emph{homogeneous} across the sequence,
so do not consider further the many constraints on protein-coding sequence \citep[reviewed in][]{thorne2007protein}.)

However, phylogenetic and population genetic methods usually ignore such dependencies
in the interest of computational efficiency and lack of understanding of the underlying process,
but some progress has been made.
Often, the effects of a complex mutational spectrum are explored using summary statistics of larger blocks or other sensible but \textit{ad hoc} methods.
For instance, \citet{arndt2003sequence} studied dinucleotide transitions,
%EM suggest going back to http://www.sciencedirect.com/science/article/pii/016747819290134L
and \citet{yaari2013models} displayed strong heterogeneity in the probability of synonymous mutations across 5-mers
in B-cell immunoglobin genes.
Others have made progress using simplifying assumptions \citep{berard2012accurate},
or by other approximations \citep{christensen2005pseudolikelihood}.
\citet{pedersen2000dependent}, later extended by
\citet{hobolth2008markov,baele2010using}, used data augmentation and an MCMC algorithm to do Bayesian inference;
\citet{lunter2004nucleotide} used an approximate matrix decomposition;
while \citet{siepel2004phylogenetic} and later \citep{baele2010modelling} computed likelihoods by assuming a model that is Markov along the genome;
however, all these methods are quite computationally intensive.

Model with fixed flanking \citet{saunders2007insights}.

Another canonical example of context-dependent transition is the Ising model of statistical physics
with time evolution given by Glauber dynamics \citep{glauber1963timedependent},
in which a lattice of up/down spins are perturbed by thermal noise,
relaxing into states dependent on the energy of the resulting configuration.
Parameter estimation for Ising model without temporal dynamics
is relatively well-undersood \citep{pickard1982inference,frigessi1990parameter},
but there does not seem to be similar work on the case when more than one observation is made of the dynamical system.

The general framework also fits certain cellular automata models,
e.g.\ modeling wildfire \citep{clarke1994cellular},
the spread of HIV \citep{zorzenondossantos2001dynamics},
or land use patterns \citep{wu2002calibration}.
Complex models may introduce long-range dependencies beyond the scope of this paper,
but these methods may still prove useful in the modeling process.




%%%%%%% %%%%%%%%%%
\section{A general context-dependent model}

Consider a 1-dimensional grid of sites, each of which can take one of a finite set of states,
and that switch randomly between states according to a local set of rules;
we then observe a finite collection of these sites at only a few times,
Suppose that the dynamics are Markov:
with the set of possible states by $\calS$,
and $X_i(t)$ the state of site $i$ at time $t$,
we assume that $\{X(t)\}_{t \ge 0}$ is a Markov process on sequences of $n$ states $\calS^n$
for which the probability a given site changes state in a small amount of time
depends only on the sequence of states at nearby sites.

To formalize this notion of local dependence,
first define a \textit{pattern} to mean a contiguous sequence of states;
let $|u|$ denote the length of the pattern $u$.
Given $x \in \calS$,
let $x_i^{(h)} = (x_i, x_{i+1}, \ldots, x_{i+h-1})$ denote the pattern of $x$ of length $h$ beginning at location $i$.
% Recall we are focusing on finite sequences, and by default, we do not allow patterns to ``hang off'' the end of the sequence.
% This is natural in many situations, but in others we might want to other boundary conditions,
% modeling extant, but unobserved, neighbors.
% We will mention this when necessary.
The dynamics of our stochastic process are determined by the set of allowed transitions and associated rates:
given two patterns $u$ and $v$ of common length $h$,
saying ``pattern $u$ changes to $v$ at rate $\mu$'' means that
\[
    \P\{ X_i^{(h)}(t+dt) = v \given X_i^{(h)}(t) = u \} = \mu dt + o(dt),
\]
regardless of the location $i$ along the sequence.
If more than one pattern would match to cause the same change, their rates add.
Such a rule is defined by its ``transition triple'' $(\mu,u,v)$,
written as ``$u \to v$ at rate $\mu$''.
The set of all transition triples is denoted $\calT$.

\begin{example}[TASEP]
  The \emph{Totally Asymmetric Simple Exclusion Process} labels each site as ``empty'' or ``occupied'' (0 or 1, respectively),
  and says that each occupied site, independently at rate $\lambda$, checks the site to the right to see if it is empty,
  and if it is, moves there.
  This process therefore has only one transition triple:

  \begin{center}
    \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }c}
      10  &   01   &  $\lambda$
    \end{tabular}
  \end{center}

  This only has one parameter, the speed.
  If we allow particles to exit from the right, and new particles to enter from the left
  (or, arrange them on a circle)
  then from observing only starting and ending configurations
  we cannot tell which particles have moved where,
  so it is not obvious how to estimate the speed.

\end{example}


\peter{Should call this ``selection'' instead of a ``potential''? Or what?}

To allow more compact specification of models,
we also introduce a ``potential'': 
suppose that there is a collection of patterns $\{p_i\}$
such that each occurrence of $p_i$ adds a quantity $e_i$ to the total ``energy'' of a sequence,
and that the rate at which each possible change occurs is modulated by a function of the energy difference the change would produce.
This is natural if that transition triples describe \emph{proposed} changes,
and that the probability a proposed change actually occurs depends on how much it affects the energy.
Concretely, let $\calP = \{(p_i, e_i\}$ be a set of (pattern, energy change) pairs,
and for any sequence $x$ define
$E(x) = \sum_i e_i n(x,p_i)$,
where $n(x,p_i)$ is the number of times the pattern $p_i$ occurs in $x$.

\begin{example}[Genomic GC content]
    A sequence of genome can be written using A, C, G, and T;
    in the most general model of independent mutation across sites, each of the 12 possible transitions occurs at its own rate.
    Furthermore, it is a well-known observation that in many species,
    adjacent, methylated CG dinucleotides (``CpG sites'') have a much higher mutation rate to TG and CA
    than either single nucleotide change under the independent model.
    Embellishing the single-nucleotide model with this additional rate results in the model defined by
    \begin{center}
      \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }cc}
        $x$  &  $y$  &  $m_{xy}$ & when $x \neq y \in \{\nA,\nC,\nG,\nT\}$  \\
        \nC\nG   &  \nT\nG   &  $\gamma$ & \\
        \nC\nG   &  \nC\nA   &  $\gamma$ &
      \end{tabular} ,
    \end{center}
    where $\gamma$ is the additional CpG rate above the base mutation rate.
    Note that for a given pair of sequences, there may be more than one way to mutate a given string $x$ to get $y$:
    for instance, a change $\nA\nC\nG \to \nA\nT\nG$ could have occurred by a single $\nC \to \nT$ mutation,
    or by a $\nC\nG \to \nT\nG$ mutation;
    and hence the total instantaneous rate at which ACG changes to ATG is $\gamma + m_{\nC\nT}$.
    % Although this means there are many possible parameterizations,
    % note that the model is identifiable, not overparameterized.

    Counteracting this trend towards increased G/C is GC-biased gene conversion \citep{gcbiased},
    which is effectively a selective pressure against G or C bases.
    We model the evolution of a \emph{single} sequence, not a population of sequences,
    imagining this sequence to be the consensus sequence of the population.
    A new mutation with an effect $s$ on fitness that occurs
    in a population of effective size $N$ becomes ubiquitous, rather than dying out,
    with probability approximately $(1-\exp(-2 s))/(1-\exp(-2 s N))$ \citep{fixation_prob}.
    If the mutation occurs at rate $\mu$ per individual, the total rate it occurs at in the population is $\mu N$.
    GC-biased gene conversion effectively means that sequences with more G/Cs are selected against.
    This is incorporated as follows: set
    \begin{center}
        \begin{tabular}{cc}
        pattern  &  energy \\
        \nC{} or \nG   &  $s$   
      \end{tabular} ,
    \end{center}
    so that $E(y)-E(x)$ is the net change in number of G's and C's multiplied by $s$.
    Also, let
    \begin{align*}
        \phi(e) = N (1-\exp(-2e))/(1-\exp(-2eN))
    \end{align*}
    so that if a given change that occurs with rate $\mu$ would change a sequence $x$ to $y$,
    then $\mu \phi(E(y)-E(x))$ is the rate at which the mutation appears and successfully takes over in the population.

\end{example}



\begin{example}[1-D Ising model with Glauber dynamics]
    In the Ising model, each site is labeled as either ``up'' or ``down'' ($+1$ or $-1$ respectively),
    imagined as magnetic dipoles,
    and the energy associated with a given state $x$ is $E(x) = - \frac{1}{2} \beta \sum_i x_i x_{i+1} - \frac{1}{2} \gamma \sum_i x_i$.
    Here $\beta$ represents inverse temperature, and $\gamma$ represents the strength of the magnetic field (here scaled by temperature).
    The associated stationary distribution on configurations is proportional to $\exp(-E(x))$.
    The following dynamics preserve the stationary distribution:
    each site, independently at rate $\lambda$,
    forgets its spin,
    and reconfigures to a state chosen with probability proportional to the stationary probability of the resulting configuration:
    Since $\exp(-E(x))/(\exp(-E(x))+\exp(-E(y))) = 1/(1+exp(-(E(x)-E(y)))$,
    this is:
    \begin{center}
        \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }c}
          $+$  &   $-$   &  $\lambda$ \\
          $-$  &   $+$   &  $\lambda$ 
        \end{tabular} 
        \qquad and \qquad
        \begin{tabular}{cc}
        pattern  &  energy \\
        $+-$ or $-+$  &   $\beta$ \\
        $+$ &   $\gamma$
        \end{tabular} 
    \end{center}
    and 
    \begin{align*}
        \phi(e) = \frac{1}{1+\exp(-e)} .
    \end{align*}

    This process has been studied by XXX.
\end{example}


\paragraph{The generator matrix}
For clarity, we describe how the set of transition rates on patterns determines the transition rate matrix for complete sequences, that is,
the $|\calS|^n \times |\calS|^n$ matrix $G(n)$ whose $(x,y)^\text{th}$ entry gives the instantaneous rate
with which the process in state $x \in \calS^n$ jumps to state $y \in \calS^n$.
Recall that $x_i^{(h)} = (x_i, x_{i+1}, \ldots, x_{i+h-1})$ denotes the subsequence of length $h$ beginning at location $i$.
For each $1\le i \le n$, pattern length $h > 0$, and patterns $u,v \in \calS^h$ define the relation
\[
x \xrightarrow{i,u,v} y \qquad \text{iff} \qquad \begin{cases}
  x_k = y_k \quad &\text{for } k<i \\
  x_{i+k} = u_k \quad &\text{for } 0 \le k < h \\
  y_{i+k} = v_k \quad &\text{for } 0 \le k < h,\ \text{and} \\
  x_k = y_k \quad &\text{for } k\ge i+h ,
\end{cases}
\]
i.e.\ if $x$ and $y$ match except at positions $i,i+1,\ldots,i+h-1$,
and for those positions $x$ matches with $u$ while $y$ matches with $v$.

As noted above, there may be more than one way to mutate a sequence $x$ to get $y$:
for each position $i$, let $J(i,x,y)$ be those transitions in $\calT$ that can be applied at $i$
to change $x$ into $y$, i.e.
\[
J(i,x,y) = \{ (\mu^j,u^j,v^j) \in \calT \st x \xrightarrow{i,u^j,v^j} y \}.
\]
The rate $G(n)_{x,y}$ is then the sum of all matching transition rates,
namely
\begin{align} \label{eqn:G_defn}
    G(n)_{x,y} = \sum_{i=1}^n \phi(E(y)-E(x)) \sum_{J(i,x,y)}  \mu^j ,
\end{align}
and if there are no triples $(\mu,u,v)$ with $x \xrightarrow{i,u,v} y$ for some $i$, then $G(n)_{x,y}=0$.
This formalizes the statement that the transition rate from $x$ to $y$ is the sum of the rates of all transition triples
that would change $x$ into $y$.
In principle, this gives us the transition probabilities for the process by a matrix exponential:
\begin{align} \label{eqn:full_likelihood}
    p_n(t;x,y) := \P\{ X(t) = y \mid X(0) = x \} = \left(e^{tG(n)}\right)_{xy} ,
\end{align}
which would then provide a route to parameter estimation.
In practice, the size of these matrices makes direct application obviously impractical.
This paper proposes an alternate strategy.


\section{Simulation}

First, it will be useful for later proofs to have an explicit construction of the process,
also used for simulation,
using a variant of the ``jump chain representation'', also known as the ``Gillespie algorithm''.
Briefly, this  works by first sampling a homogeneous Poisson process of times and locations of possible changes,
at an appropriate rate $\mu_*$ per site,
and then resolving each possible change in temporal order.
The rate $\mu_*$ should be the maximum ``local'' rate at which transitions occur, across sites and transition outcome states:
\[
\mu_* = \max \left\{ \sum_{y \in \calS^n} \sum_{j\in J(i,x,y)} \mu_j \qquad \st x \in \calS^n, \quad 1 \le i \le n \right\} .
\]
Note that since the process is homogeneous, the maximum rate of changes occurring at any position will be the same, except possibly near the end of the sequence,
so the maximization over $i$ is unnecessary.
Now sample possible changes: let $c_{ij}$ denote the time of the $j^\text{th}$ possible change at site $i$,
so that for each site $i$,
\[
0 \le c_{i1} < c_{i2} < \cdots < c_{iN_i} \le t
\]
is a draw from a rate $\mu_*$ Poisson process on $[0,t]$.
These processes are independent across sites.
Now, suppose we have determined the state at time $s$ to be $X(s) = x$,
and the next event is $c_{ij} = \min_{k,h} \{ c_{kh} : c_{kh}>s \}$.
At $c_{ij}$,
a new state is then chosen at position $i$
with probability proportional to the corresponding rate, i.e.\
\erick{Previously, the $j$ in $\mu^j$ was used to index the possible patterns. Now it's indexing possible changes at a site.}
\[
q_j = \begin{cases}
  \mu_j/\mu_* \qquad & \text{if } x_i^{(|u^j|)} = u^j  \\
  0 \qquad & \text{otherwise,}
\end{cases}
\]
is the probability that $x_i^{(|u^j|)}$ is replaced with $v^j$.
The remaining probability $q_0 = 1-\sum_{j=1}^{|\calT|} q_j$ gives the probability that the state remains the same.
(By construction of $\mu_*$, $q_0\ge 0$.)

For instance, to simulate TASEP, one has only to let $\mu_*=\lambda$, and at each $c_{ij}$
check if at site $i$ there is a 1 followed by a 0,
and if so, switch them.


\section{Inference}

\peter{shorten, refer to figure}

The problem at hand is to infer the parameters of the model based on the final state of the model after it hsa evolved from some known initial state.
% If the sequence is long enough and not too much time has gone by
% this should be feasible -- there should be enough information in our observations to do this reliably.
% If the time is too large, the process may ``saturate'' --
% if there are many changes at most sites, then in at least most models,
% we will lose most information about the dynamics,
% retaining information only about the parameters that affect the stationary distribution.
% In TASEP with any boundary conditions, we lose all information as it approaches stationarity,
% while in the Ising model we have information about the temperature $\beta$ and magnetic field $\gamma$ but not the speed of changes, $\lambda$.
% In the CpG model it is not immediately clear what information is retained.
How can we extract the information?
These are Markov processes, on the state space $\calS^n$,
so the full likelihood function is given by \eqref{eqn:full_likelihood},
but doing anything with the $|\calS|^n \times |\calS|^n$ matrix $G_n$ is clearly infeasible for even moderately sized $n$.
We can, however, compute \eqref{eqn:full_likelihood} for smaller $n$,
so the first thing that one might think to do is to break the sequence up into many blocks of length $m$,
and treat these as independent.
Then, defining $p_m(t;x,y)$ to be the probability that a string $x$ of length $m$ evolves to string $y$ over time $t$,
we would obtain the approximate likelihood function
\[
  \prod_{k=0}^{n/m} p_m(x_{km+1}^{(m)},y_{km+1}^{(m)}) .
\]
However, there is no guarantee that this is even close to correct,
as it ignores dependencies between neighboring blocks.
% Intuitively, we want to increase the size of the context
% -- and the likelihood thus obtained is asymptotically correct for $m$ large,
% but computationally, we are stuck with small values of $m$.

\begin{figure}
    \begin{center}
        \includegraphics{Tmer-dependency}
    \end{center}
    \caption{
        \textbf{(A)} A $(6,2,2)$ T-mer.
        Nonmutated sites are shown in lower case letters.
        \textbf{(B)}
        The three mutations have extended the influence of the circled \nA{} 
        in the initial sequence (top)
        to the six positions colored red in the final sequence.
        Red dotted arrows show the propagation of dependency 
        between the circled \nA{}
        and the third site in the final sequence (bottom).
        The maximum window width, $w$, is equal to 1.
        \label{fig:Tmers}
    }
\end{figure}



The key insight we use is that by including neighboring sequence context in the \emph{initial} sequence only,
we can compute \emph{something} correctly,
and that something will be sufficient to compute the full likelihood.
To do this, let $p_{n,\ell,r}(x,y)$ for $r+\ell < n$ denote
the probability that an initial sequence $x$ of length $n$, after time $t$,
is found to match a smaller sequence $y$ of length $n-\ell-r$, offset by $\ell$ sites.
We refer to such patterns as ``T-mers'', as depicted in Figure \ref{fig:Tmers}.
This quantity is found by marginalizing $p_m$:
\begin{align}
    p_{n,\ell,r}(t;x,y) &= \sum_{a \in \calS^\ell} \sum_{b \in \calS^r} p_n(t;x,a \join y \join b) , \quad \text{for}\; x \in \calS^n, \; y \in \calS^{n-\ell-r} ,
\end{align}
where $a \join y \join b$ is the sequence composed of concatenating $a$, $y$, and $b$ together in that order.
We expect $p_{n,\ell,m}(x,y)$ to be asymptotically correct when $\ell$ is large (and $t$ is fixed),
i.e.\ that the probability a given subsequence matching $x$ is later observed to have its center matching $y$
is close to $p_{\ell,m}(x,y)$, regardless of context wider than $2k+l$:
\begin{align} \label{eqn:window_approx}
    p_{n,\ell,r}(x,y) \approx \P\{ X_{i+\ell}^{(n-\ell-r)}(t) = y \mid X_i^{(n)}(0) = x \}.
\end{align}

%%%%%%
\subsection{Full likelihood}

What we actually want is the \emph{full likelihood} $\like(y|x)$,
the probability that a given sequence $x$ evolves into another, $y$, over a given period of time.
This can be computed as the product of the probability of each site in $y$ given $x$ and previous sites:
defining $x_{<i}=(x_1, \ldots, x_{i-1})$, this is:
\begin{align*}
    \like(y|x) &= \prod_i \P\{ Y_i = y_i \given Y_{<i} = y_{<i} \;\&\; X=x \} .
\end{align*}
Using the same methods, we can approximate the terms in this sum
by discarding approximately independent sites from the conditioning.
If $\ell$ is long enough that approximation \eqref{eqn:window_approx} holds, then:
\begin{align} \label{eqn:full_approx}
    \P\{ Y_i = y_i \given Y_{<i} \;\&\; X \} 
    &\approx
        \P\{ Y_i = y_i \given Y_{i-2\ell}^{(2\ell+1)} \;\&\; X_{i-3\ell}^{(4\ell+1)} \}  \\
    &=
        \frac{
            p_{2\ell+1,\ell,\ell}(x_{i-3\ell}^{(4\ell+1)}, y_{i-2\ell}^{(2\ell+1)})
        }{
            p_{2\ell+1,\ell,\ell+1}(x_{i-3\ell}^{(4\ell+1)}, y_{i-2\ell}^{(2\ell)})
        } .
\end{align}
By rearranging the product, 
this gives us an expression for the full likelihood as the ratio of the likelihoods of all $(n,\ell,\ell)$ T-mers in $(x,y)$
to that of of all $(n,\ell,\ell+1)$ T-mers.
% Define $n_\ell(x,y;a,b)$ to be the number of times pattern $a$ is seen in sequence $x$
% matched to pattern $b$ in $y$ at a position offset by $\ell$.
% Then
% \begin{align}
%     \log \like(y|x) 
%     &\approx 
%         \sum_{a \in \calS^n} \sum_{b \in \calS^{n-2\ell}} n_\ell(x,y;a,b) \log p_{n,\ell,\ell}(a,b)
%         -
%         \sum_{c \in \calS^n} \sum_{d \in \calS^{n-2\ell-1}} n_\ell(x,y;c,d) \log p_{n,\ell,\ell+1}(c,d)
% \end{align}

%%%%%
\subsection{Choice of overhang}

The approximations in \eqref{eqn:window_approx} and \eqref{eqn:full_approx} hold if the overhang, $\ell$, is long enough.
The intuition is straightforward, and provides a way to pick the overhang lengths $\ell$ and $r$;
a precise statement is given in Section \ref{ss:approx_pf}.
Suppose that each instantaneous change depends on positions at most $w$ sites away.
Then, the mutation process at any particular site
depends on the initial sequence at that site and within distance $w$ on either side, 
and the outcomes of any mutations that have occurred within that window.

Therefore, the sequence at a particular site may directly affect the process at the neighboring $w$ sites on either side.
If another change happens, say, at the end of this window,
then the indirect effects of this change may extend out to $2w-1$ sites.
In this way, the influence of the sequence at each site extends outwards, as shown in Figure \ref{fig:Tmers} --
so if we take the overhangs long enough that this dependency is unlikely to extend to the base of the Tmer,
the approximation will be good.
As we show in Section \ref{ss:approx_pf}, if the overhangs are $\ell = r = kw$,
then the error in approximation \eqref{eqn:window_approx}
is bounded by the probability that there are more than $k$ changes in a given window of length $w$.
Concretely, sites further away than $kw$ only matter if there is a chain of at least $k$ intervening mutations
-- which is unlikely if $k$ is larger than the expected number.

For instance, in the CpG model the maximum dependency is $w=1$ as the process only depends on neighboring sites;
and if the sequence has evolved for time $t$ then we expect no more than a Poisson($\lambda t$) number of changes at each site.
If $\lambda t = 0.75$ (three-quarters of the sequence will have changed!),
then taking $\ell = r = 3$ allows us to compute Tmer probabilities to within an error of 0.007,
since this is the probability that a Poisson with mean 0.75 will be larger than 3.
If $\lambda t = 0.25$ we would do even better with $\ell = r = 2$.

To obtain the full likelihood, in \eqref{eqn:full_approx}, in addition to an overhang of $\ell$
we needed the base of the T-mer to be of length $2\ell+1$.
This is because we have also conditioned on the final sequence to the left of the focal site, $Y_{<i}$,
and although the process at $i$ only depends on events within $\pm \ell$,
these events may affect other sites in $Y$ going back a distance $2\ell$.


%%%%%%%%
\subsection{Model fit and model selection}

The difference between observed and expected T-mer counts provide a good assessment of how and whether a model can be improved.
These \emph{residual} counts are usually most usefully computed for T-mers shorter than those used to fit the model.
For each T-mer $(x,y)$, write $O_{x,y}$ for the number of this T-mer observed in the data, 
and $E_{x,y}$ for the number expected under the model.
(The expected number is the number of occurrences of $x$ in the initial sequence 
multiplied by the probability $p_{n,\ell,r}(t;x,y)$, where $n$ is the length of $x$,
and $\ell$, $r$ are the appropriate overhangs.)
If the T-mers were nonoverlapping, this would form a standard contingency table,
and standard practice would be to divide the residuals by the square root of the expected counts
and compare these to a standard Normal distribution.
However, there are are $n$ possible such tables --
so, we compute normalized residuals as
\begin{align}
    Z_{x,y} = \frac{ O_{x,y} - E_{x,y} }{ \sqrt{n E_{x,y}} },
\end{align}
which is equivalent to averaging the $z$-scores of the $n$ possible tables of nonoverlapping counts.
These can then be compared to a standard Normal distribution.

%%%%%%%%%%%
\section{Computation}

For inference, we need to compute the $|\calS|^{m+2\ell} \times |\calS|^m$ matrix $F$ whose $(x,y)^\text{th}$ entry is $p_{m,\ell}(t,x,y)$,
for various $t$.
This matrix is a projection of the $|\calS|^{m+2\ell} \times |\calS|^{m+2\ell}$ matrix whose $(x,z)^\text{th}$ entry is
\begin{align}
    p_{m+2\ell}(x,z) = \left( e^{t G(m+2\ell)} \right)_{x,z} ,
\end{align}
where $G(m+2\ell)$ is the sparse $|\calS|^{m+2\ell} \times |\calS|^{m+2\ell}$ matrix defined in \eqref{eqn:G_defn}.
The ``projection'' we need just marginalizes over long patterns $z$ that match the shorter pattern $y$:
if we define the matrix $U$ in this way, so that $U_{zy}=1$ if $z_\ell^m=y$ and $U_{zy}=0$ otherwise,
then
\begin{align} \label{eqn:Tmer_trans}
    p_{m,\ell}(t,x,y) = \sum_{z \in \calS^{m+2\ell}} \left( e^{t G(m+2\ell)} \right)_{xz} U_{zy} ,
\end{align}
so in fact we don't need the entire matrix $e^{t G(m+2\ell)}$,
just the product of this matrix with each of the columns of $U$.
Modern techniques in sparse matrix computation (e.g., Krylov methods) provide efficient ways to do this.

Using the R package expm \citep{R_expm}, this makes computation quite feasible:
with four possible states and $\ell=2$ and $m=1$, so that $G(m+2\ell)$ is a $1024 \times 1024$ matrix,
computing $e^{t G(5) }$ takes 13 seconds, while computing $e^{t G(5)} U$ takes only 0.3 seconds.
Increasing to $\ell=4$ and $m=1$ or $\ell=3$ and $m=2$ is still feasible, taking $e^{t G(8)} U$ in 42 and 47 seconds, respectively
(and much longer for the whole matrix $e^{t G(8)}$).

\paragraph{Sparsity and updating $G$}
We can also take advantage of the sparsity of $G$ to perform efficient computation of the likelihood
under many sets of parameters.
First note that $G(m+2\ell)$ has, assuming only single-position changes,
$(1+m(|\calS|-1)) |\calS|^{m+2\ell}$ nonzero entries, since each of the $|\calS|^{m+2\ell}$ can change in $m$ places.
This will determine how the computation scales with $m$, $\ell$, and $|\calS|$,
once we precompute a number of things.
Let $g = (g_1, \ldots, g_d)$ be the nonzero entries of $G$, in some fixed ordering;
sparse matrix representations of $G$ store only $g$ along with information about the rows and columns these are found in.
Each $g_i$ is a linear combination of mutation rates $\mu_j$,
multiplied by a function ($\phi$) of a linear combination of energy coefficents $e_j$, 
say $g_i = \sum_j A_{ij} \mu_j \phi(\sum_k B_{ik} e_k)$,
so by precomputing the matrices $A$ and $B$ we can update $G$ with new parameter values
using only two matrix multiplications and evaluation of $\phi()$ over a vector.
This remains efficient to perform in interpreted languages such as R,
since each step is carried out by lower-level compiled code (e.g., optimized linear algebra libraries).
% This is true if the boundary conditions are circular, mean-value, or neither.

%%%%%%%%
\section{Phylogenetic inference}

In phylogenetic applications, rather than ``before'' and ``after'' observations,
we get two (or more) observations evolved from a common root.
In the simplest case of two tips we have two processes $X$ and $Y$,
with identical starting states $X(0)=Y(0)$,
observed only at times $t_X$ and $t_Y$ respectively.

If the approximation described above holds,
then we can write the probability that we see (long) pattern $x$ in $X(t_X)$ juxtaposed with (short) pattern $y$ in $Y(t_Y)$
using the pattern frequencies at the root.
Concretely: pick a random location $I$ in the sequence and let $X_{I-\ell}^{m+2\ell}(0) = \rho$ be the (long) pattern seen there at the root,
and write $\pi(z)$ for the frequency of pattern $z$ in $X(0)$, i.e.\ $\P\{\rho=z\}=\pi(z)$.
% Given the corresponding pattern in $X(t_X)$, the distribution of $\rho$ is proportional to
% \[
%     \P\{ \rho=z \mid X_{I-\ell}^{m+2\ell}(t_X)=x\} \propto \pi(z) p_{m+2\ell}(t,z,x) .
% \]
The probability that we see $x$ and $y$ at the random location $I$ is
\begin{align} \label{eqn:phylo_likelihood}
    \P\{X_{I-\ell}^{m+2\ell}(t_X)=x \text{ and } Y_I^\ell(t_Y)=y \} = \sum_{z \in \calS^{m+2\ell}} \pi(z) p_{m+2\ell}(t_X,z,x) p_{m,\ell}(t_Y,z,y) .
\end{align}
This can be computed without much more effort than the simpler case above,
given the frequencies at the root, as described in greater generality below.
These calculations can be done using the the same sparse matrix methods as above.
For instance, to compute
\[
    \sum_z \pi(z) p_{m+2\ell}(t_X,z,x) p_{m,\ell}(t_Y,z,y)
    = \sum_z \left( e^{t_X G(m+2\ell)} \right)_{zx} \pi(z) \sum_w \left( e^{t_X G(m+2\ell)} \right)_{zw} U_{wy} ,
\]
we can first compute $\sum_w \left( e^{t_X G(m+2\ell)} \right)_{zw} U_{wy}$ as above,
multiply rows by $\pi(z)$, and then matrix multiply by the transpose of $G(m+2\ell)$.


\subsection{Phylogenetic peeling}

The general case is a modification of Felsenstein's ``peeling'' algorithm.

\begin{figure}
    \begin{center}
    \includegraphics{peeling-schematic}
    \end{center}
    \caption{
        A depiction of the four steps in computing the likelihood of finding particular ``short'' subsequence at $Y$ and $Z$
        given the ``long'' subsequence at $X$: see text for details.
        \label{fig:peeling}
    }
\end{figure}

To compute the likelihood on a more complicated tree, consider the following,
with the three-taxon tree of figure \ref{fig:peeling} as an example,
where we are counting occurrences of (long) $m+2\ell$-tuples in taxon $X$,
and (shorter) $m$-tuples in taxa $Y$ and $Z$.
The likelihood in this example is found by summing over the root state (the summation over $u$)
and the interior node ($v$):
\begin{align} \label{eqn:three_taxa_likelihood}
  \begin{split}
    & \P\{X_{I-\ell}^{m+2\ell}=x \text{ and } Y_I^\ell=y \text{ and } Z_I^\ell=z \} \\
    &\qquad = \sum_{u \in \calS^{m+2\ell}} \pi(u) p_{m,\ell}(t_C,u,z) \sum_{v \in \calS^{m+2\ell}} p_{m+2\ell}(t_{AB},u,v) p_{m+2\ell}(t_{A},v,x) p_{m,\ell}(t_B,v,y)
  \end{split}
\end{align}
The steps for computing this are then:
\begin{enumerate}

  \item Compute $M_1(u,z) = p_{m,\ell}(t_C,u,z)$ (a $|\calS|^{m+2\ell} \times |\calS|^m$ matrix)

  \item Compute $M_2(v,z) = \sum_{u \in \calS^{m+2\ell}}  p_{m+2\ell}(t_{AB},u,v) \pi(u) M_1(u,z)$ (still a $|\calS|^{m+2\ell} \times |\calS|^m$ matrix)

  \item Compute $M_3(v,y) = p_{m,\ell}(t_B,v,y)$ (a $|\calS|^{m+2\ell} \times |\calS|^m$ matrix)

  \item Compute $M_4(x,(y,z)) = \sum_{v \in \calS^{m+2\ell}} p_{m+2\ell}(t_{A},v,x) M_2(v,z) M_3(v,y)$ (stored as a $|\calS|^{m+2\ell} \times |\calS|^{2m}$ matrix).

\end{enumerate}
Then $M_4$ gives all the likelihoods.
This is depicted in the figure, writing $\left(e^{tG}\right)_{ab}$ for the matrix $p_{m+2\ell}(t,a,b)$,
and $P$ for the projection matrix that gives $p_{m,\ell}(t,a,b) = \left( e^{tG} P\right)_{ab}$.


In total there are $|\calS|^{3m+2\ell}$ possible data combinations.
However, we can reduce dimensionality by replacing the last step with
\begin{enumerate}

  \item[4'.] Compute $M_4(x,w) = \sum_{v \in \calS^{m+2\ell}} p_{m+2\ell}(t_{A},v,x) M_3(v,w) M_4(v,w)$ (stored as a $|\calS|^{m+2\ell} \times |\calS|^{m}$ matrix).

\end{enumerate}
i.e., only computing the likelihood for cases with $y=z=w$.
On a three-taxon tree it is not clear that this is desireable, since missing out on double transitions
might be a significant loss of information,
but on larger trees it seems likely that we'd want to restrict to combinations with most sites agreeing across the tree.
(Note, however, that one should not first look at the data, observe which combinations $(y,z)$ were most common, and then restrict to only those!)

The algorithm for a general tree is written out in Appendix \ref{ss:peeling_algorithm}.


%%%%%%%
\section{Results}

\textbf{To-do:} 

3. Analyze human data with shape model, Bayesian.

The method works well to find point estimates of the parameters.
Obtaining posterior density estimates using the approximation likelihood function on a subset of patterns
works in some cases;
in other cases the likelihood surface is still too peaked, underestimating the uncertainty.
(but counting in nonoverlapping windows fixes the peakiness problem)

%%%%%%%%%%%
\subsection{Stepwise model selection}

In exploratory application to real data, how does one decide when to stop adading mutational motifs?
Perhaps the simplest method conceptually is to 
(a) fit a simple model,
(b) choose new motifs to add based on statistically significant residuals,
(c) repeat until no residuals are statisically significant.
To test this method,
we simulated a single sequence of $10^6$ nucleotides
for one units of time
under the model given in Table \ref{tab:cpg_results},
obtained by embellishing the basic CpG hypermutability
with the longer mutational motif identified by \citet{harris}.
%     \begin{center}
%       \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }cc}
%         $x$  &    $y$  &  $m_{xy}$ & when $x \neq y \in \{\nA,\nC,\nG,\nT\}$  \\
%         \nC\nG     &  \nT\nG     &  $0.4$ & \\
%         \nC\nG     &  \nC\nA     &  $0.4$ & \\
%         \nT\nC\nC  &  \nT\nT\nC  &  $0.1$ & \\
%         \nA\nG\nG  &  \nA\nA\nG  &  $0.1$ &
%       \end{tabular} ,
%     \end{center}
% with $m_{AT}=m_{TA}=0.1$, $m_{CG}=m_{GC}=0.15$, $m_{AC}=m_{TG}=m_{AG}=m_{TC}=0.08$, and $m_{CA}=m_{GT}=m_{CT}=m_{GA}=0.12$.
% Note the model is symmetric under reverse complementation.
The resulting sequences differed at 28.5\% of the sites.
We then fit an unconstrained model of single-nucleotide substitution using (5,1,1) T-mers
(three sites at the base with an overhang of one on each side)
using a constrained quasi-Newton method (``optim(.., method='L-BFGS-B')'' in R).
The top and bottom of the table of (2,1,0) and (2,0,1) T-mer residuals of this model were as follows:
% cpg-plus-epsilon/32140/base-model-fit-5-3-l1-resids.2.1.l0.tsv
% cpg-plus-epsilon/32140/base-model-fit-5-3-l1-resids.2.1.l1.tsv
    \begin{center}
        \begin{tabular}{ccrrrr}
                $x$ & $y$ & observed &   expected &    residual &  $Z$ \\
                \hline
                \nC\nG  & \_\nC   & 100410   &  120303    & -19892.50   &	-40.55  \\
                \nC\nA  & \_\nT   &  19884   &   28350    &  -8466.43   &	-35.55  \\
                \nC\nT  & \_\nT   &  19998   &   28061    &  -8063.46   &	-34.03  \\
                \nC\nC  & \_\nT   &  23673   &   28177    &  -4503.96   &	-18.97  \\
                \nG\nG  & \_\nC   &  17742   &   19418    &  -1675.92   &	 -8.50  \\
                \nC\nG  & \_\nG   &  18450   &   19478    &  -1028.34   &	 -5.21  \\
                \nA\nG  & \_\nC   &  10968   &   11760    &   -791.73   &	 -5.16  \\
                \nG\nG  & \_\nG   & 118805   &  120975    &  -2170.25   &    \\
                \hline
                \nA\nG  & \_\nT   &  17280   &   16515    &    765.20   &  4.21   \\
                \nG\nT  & \_\nC   &  20490   &   19507    &    983.44   &  4.97   \\
                \nG\nG  & \_\nT   &  20929   &   19278    &   1650.83   &  8.40   \\
                \nC\nC  & \_\nC   & 124149   &  119880    &   4268.52   &  8.71   \\
                \nG\nG  & \_\nA   &  30483   &   28288    &   2195.34   &  9.22   \\
                \nC\nT  & \_\nC   & 126957   &  119389    &   7567.92   & 15.48   \\
                \nC\nA  & \_\nC   & 128673   &  120619    &   8054.46   & 16.39   \\
                \nC\nG  & \_\nT   &  49311   &   28276    &  21034.85   & 88.45   \\
                \hline
                \hline
                \nC\nG  &  \nG\_  &  100803  &  120541  &  -19737.80  &  -40.19 \\
                \nT\nG  &  \nA\_  &   20040  &   28179  &   -8139.45  &  -34.28 \\
                \nG\nG  &  \nA\_  &   20985  &   28288  &   -7302.96  &  -30.70 \\
                \nA\nG  &  \nA\_  &   22929  &   28399  &   -5469.73  &  -22.95 \\
                \nC\nC  &  \nG\_  &   17805  &   19410  &   -1605.01  &   -8.14 \\
                \nC\nT  &  \nG\_  &   10470  &   11684  &   -1214.31  &   -7.94 \\
                \nC\nA  &  \nG\_  &   10839  &   11882  &   -1043.39  &   -6.76 \\
                \nC\nG  &  \nC\_  &   18354  &   19348  &    -994.19  &   -5.05 \\
                \hline
                \nC\nT  &  \nA\_  &   17007  &   16078  &     929.33  &    5.18 \\
                \nT\nC  &  \nG\_  &   20691  &   19509  &    1182.40  &    5.98 \\
                \nC\nC  &  \nA\_  &   20640  &   19160  &    1480.44  &    7.56 \\
                \nT\nC  &  \nT\_  &   30309  &   28320  &    1988.91  &    8.35 \\
                \nA\nG  &  \nG\_  &  127191  &  121450  &    5740.76  &   11.64 \\
                \nG\nG  &  \nG\_  &  127946  &  120977  &    6969.46  &   14.16 \\
                \nT\nG  &  \nG\_  &  127539  &  120512  &    7026.52  &   14.31 \\
                \nC\nG  &  \nA\_  &   49098  &   28186  &   20911.93  &   88.07 \\
                \hline
        \end{tabular}
    \end{center}
The situation clearly calls for addition of the CpG mutation motif.
After adding these patterns into the model and re-fitting,
% starting at the parameters inferred from the first run and a CpG rate of 0.0,
some length 2 residuals had statistically significant residuals,
but none so large as those seen in (3,1,1) T-mer residuals, shown here:
    \begin{center}
        \begin{tabular}{ccrrrr}
                $x$ & $y$ & observed &   expected &    residual &  $Z$ \\
                \hline
                \nA\nG\nG  &  \_\nG\_  &  30024  &  32167  &  -2143.48  &  -6.90 \\
                \nT\nC\nC  &  \_\nC\_  &  29910  &  31748  &  -1838.13  &  -5.95 \\
                \nG\nG\nT  &  \_\nA\_  &   5064  &   5549  &   -484.61  &  -3.75 \\
                \nT\nG\nC  &  \_\nA\_  &   4830  &   5172  &   -342.42  &  -2.74 \\
                \nG\nG\nA  &  \_\nA\_  &   5115  &   5462  &   -346.97  &  -2.71 \\
                \nC\nT\nT  &  \_\nG\_  &   2472  &   2717  &   -244.56  &  -2.70 \\
                \nG\nC\nT  &  \_\nT\_  &   4863  &   5192  &   -328.56  &  -2.63 \\
                \hline
                \nA\nT\nG  &  \_\nG\_  &   3225  &   3028  &    196.78  &   2.06  \\
                \nA\nC\nG  &  \_\nA\_  &   4830  &   4588  &    242.29  &   2.06  \\
                \nT\nG\nT  &  \_\nC\_  &   5364  &   5105  &    258.74  &   2.09  \\
                \nG\nA\nG  &  \_\nG\_  &   3285  &   3055  &    229.78  &   2.40  \\
                \nT\nT\nG  &  \_\nC\_  &   2973  &   2752  &    220.59  &   2.42  \\
                \nT\nC\nT  &  \_\nG\_  &   5421  &   5056  &    364.54  &   2.95  \\
                \nT\nC\nC  &  \_\nT\_  &   7590  &   5500  &   2089.79  &  16.26  \\
                \nA\nG\nG  &  \_\nA\_  &   7566  &   5263  &   2302.74  &  18.32  \\
                \hline                                                     
        \end{tabular}
    \end{center}
\ldots which again clearly call for adding the mutation tuples $TCC \to TTC$ and $AGG \to AAG$.
After adding these in, 
% and initializing all parameters to 0.1 
no further (3,1,1) T-mer residuals achieved
statistical significance after a Bonferroni correction;
the final parameter values are shown in Table \ref{tab:cpg_results};
parameter estimates differ from the truth by less than 3\% (mostly within 1\%).


% df <- structure(list(
%     sim = c(0.1, 0.1, 0.15, 0.15, 0.08, 0.08, 0.08, 0.08, 0.12, 0.12, 0.12, 0.12, 0.4, 0.1), 
%     fit = c(0.0996, 0.0982, 0.1505, 0.1497, 0.0784, 0.0792, 0.0798, 0.0795, 0.1202, 0.1196, 0.1198, 0.1193, 0.3978, 0.1033)), 
%     .Names = c("sim", "fit"), 
%     row.names = c("A->T", "T->A", "C->G", "G->C", "A->C", "T->G", "A->G", "T->C", "C->A", "G->T", "C->T", "G->A", "CG->TG|CG->CA", "TCC->TTC|AGG->AAG"), 
%     class = "data.frame")

\begin{table}
  \begin{center}
        \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }ll}
          \hline
            x & y & truth & fitted \\ 
          \hline
          \nA  &   \nT           & 0.10 & 0.0996 \\ 
          \nT  &   \nA           & 0.10 & 0.0982 \\ 
          \nC  &   \nG           & 0.15 & 0.1505 \\ 
          \nG  &   \nC           & 0.15 & 0.1497 \\ 
          \nA  &   \nC           & 0.08 & 0.0784 \\ 
          \nT  &   \nG           & 0.08 & 0.0792 \\ 
          \nA  &   \nG           & 0.08 & 0.0798 \\ 
          \nT  &   \nC           & 0.08 & 0.0795 \\ 
          \nC  &   \nA           & 0.12 & 0.1202 \\ 
          \nG  &   \nT           & 0.12 & 0.1196 \\ 
          \nC  &   \nT           & 0.12 & 0.1198 \\ 
          \nG  &   \nA           & 0.12 & 0.1193 \\ 
       \nC\nG  &  \nT\nG         & 0.40 & 0.3978 \\ 
       \nC\nG  &  \nC\nA         & 0.40 & 0.3978 \\ 
    \nT\nC\nC  &  \nT\nT\nC      & 0.10 & 0.1033 \\ 
    \nA\nG\nG  &  \nA\nA\nG      & 0.10 & 0.1033 \\ 
           \hline
        \end{tabular}
  \end{center}
  \caption{ Results from iterative fitting to a $10^6$ sequence evolved to have 28.5\% sequence divergence
    with the above mutational motifs. The transitions $CG \to TG$ and $CG \to CA$ 
    were constrained to be equal (DNA strand symmetry),
    as were the transitions $TCC \to TTC$ and $AGG \to AAG$.
    \label{tab:cpg_results} }
\end{table}


%%%%%%%%%%%
\subsection{Bayesian inference}

The usage of fast sparse matrix methods described above make likelihood computation fast enough
to use a Metropolis-Hasting-based Markov chain Monte Carlo method
to sample from the posterior distribution on parameters.
To test this,
we evolved $10^6$ sites from the Ising model (described above) with $\lambda=0.5$, $\beta=1.0$, and $\gamma=0.5$
for 1 unit of time, 
resulting in around 460,000 possible mutations
and differences at around 186,000 sites between initial and final sequence.
We then placed an Exponential prior with mean 1 on both transitions $+ \to -$ and $- \to +$
(without constraining these to be equal)
and half-Gaussian priors with mean 0 and scale 3 on both $\beta$ and $\gamma$,
counted $(5,3,1)$ T-mers,
and used the mcmc package \citep{mcmcR} to run a random walk sampler for 10,000 steps
(judged sufficient by examination of diagnostic plots).
We repeated this procedure separately on 60 independently simulated sequences,
and show the resulting credible intervals in Figure \ref{fig:ising_coverage}.
Poseterior medians are within half a percent of the truth for $+ \to -$ and $- \to +$ (corresponding to $\lambda$)
and the temperature ($\beta$), but showed around 2\% error for the magnetic field $\gamma$.
As seen in Table \ref{tab:ising_coverage}, 
credible intervals were somewhat too narrow for shorter T-mers,
but $(6,2,2)$ T-mers were long enough with sufficient overhang to produce well-calibrated posteriors.

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
    & (4,1,1) & (5,1,1) & (6,2,2) \\ 
  \hline
  $\lambda_-$ & 0.61 & 0.63 & 0.96 \\ 
  $\lambda_+$ & 0.87 & 0.86 & 0.85 \\ 
  $\beta$     & 0.76 & 0.81 & 0.94 \\ 
  $\gamma$    & 0.69 & 0.76 & 0.85 \\ 
   \hline
\end{tabular}
    \caption{
        Posterior coverage of the four parameters of the dynamic Ising model at four different T-mer sizes.
        Shown are the proportion of runs in which the true value fell within the 95\% credible interval,
        for T-mers of length 4, 5, and 6 with overhangs of length 1, 1, and 2 respectively.
        The total number of runs was 61 for $(6,2,2)$ T-mers and 121 for the others.
        \label{tab:ising_coverage}
    }
\end{table}

\begin{figure}
    \begin{center} 
        \includegraphics{writeup-plots/coverage_results}
    \end{center} 
    \caption{
        Credible intervals for the four parameters of the dynamic Ising model,
        calculated using $(6,2,2)$ T-mers,
        from 61 MCMC runs on independently simulated sequences of $10^6$ bases
        differing at around 18.6\% of the sites.
        Shown are relative intervals, obtained by dividing by the true value.
        \label{fig:ising_coverage}}
\end{figure}


%%%%%%%%%%%
\subsection{TASEP}

Strings of length $10^5$ were evolved under TASEP for varying amounts of time,
and pattern occurrences counted.
Running 10,000 MCMC iterations with window sizes of $\ell=w=3$ (total length 9) on these takes around 40 minutes.
Apparently, the patterns used were not sufficiently independent in this case,
as the posterior is too sharp.

Here's a short example sequence with $\lambda=1$:
\begin{center}
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc}
X&X&X&O&O&X&O&O&O&O&X&O&X&O&X&X&O&O&X&O&O&X&O&X&X&X&O&X&O&O&O&X&X&O&X&O&O&O&X&X&O&O&O&X&O&O&O&O&X&O&X&X&O&X&X&O&O&X&O&O \\
$\centerdot$&O&X&O&X&O&O&O&X&$\centerdot$&O&X&O&O&X&X&X&o&x&$\centerdot$&o&O&X&x&x&O&X&O&X&o&o&O&X&X&$\centerdot$&$\centerdot$&o&o&O&X&X&o&$\centerdot$&O&X&$\centerdot$&o&o&$\centerdot$&$\centerdot$&O&X&X&x&O&X&o&O&X&$\centerdot$
\end{tabular}
\end{center}
here particles are X's, moving to the right; and spaces are O's;
in the second line,
$\centerdot$ is a position that has not considered a change; lowercase letters are positions that considered a change but did not;
and upper case are positions that did change.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{writeup-plots/tasep_all-mcmc-runs}
  \end{center}
  \caption{ Results from MCMC estimates for six different data sets of the parameter in TASEP (mutation rate multiplied by time).
  Vertical line shows the true value.
  }
\end{figure}


\subsection{Ising model}

Strings of length $10^6$ were evolved under Glauber dynamics on the Ising model with varying parameters,
and pattern occurrences counted.
As for TASEP, running 10,000 MCMC iterations with window sizes of $\ell=w=3$ (total length 9) on these takes around 40 minutes.
In this case, the true parameter values have good distribution within the estimated posterior distributions.

Here is an example, with all parameters equal to 1, for $t=.5$:
\begin{center}
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc}
X&X&O&X&X&O&O&X&O&X&O&X&X&O&O&X&O&O&O&O&X&X&X&O&X&O&X&O&X&O&O&O&X&X&O&X&X&X&X&X&O&X&O&X&O&X&X&X&X&O&O&X&X&X&X&X&X&O&O&O \\
x&x&o&x&X&X&O&x&o&x&$\centerdot$&x&x&O&X&X&$\centerdot$&O&X&O&X&x&x&o&x&o&x&o&x&O&X&O&$\centerdot$&$\centerdot$&O&O&X&X&O&X&X&X&o&x&o&$\centerdot$&X&O&X&o&o&x&X&X&X&O&X&o&$\centerdot$&$\centerdot$
\end{tabular}
\end{center}
The notation is as above; except that the entire matched pattern involved in a change is uppercase:
to simulate, we look in windows of size 3; so if XOO changes to XXO, the whole thing is uppercase, not just the middle X.

\begin{figure}
  \begin{center}
    \includegraphics{writeup-plots/selsims-2013-05-28-17-12-0275615-traces}
  \end{center}
  \caption{
  MCMC traces of the parameters in the Ising model across 100,000 iterations.
  The units of $\lambda$ are in mean number of changes per site.
  Vertical line shows the true value.
  }
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth,page=1]{writeup-plots/ising_all-mcmc-runs}
  \end{center}
  \caption{ Results from MCMC estimates of the time-scaled mutation rate parameter $\lambda t$,
  in ten different simulated datasets under the Ising model.
  Vertical line shows the true value.
  }
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth,page=2]{writeup-plots/ising_all-mcmc-runs}
  \end{center}
  \caption{ Results from MCMC estimates of the inverse temperature $\beta$ in the Ising model.
  Vertical line shows the true value.
  }
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth,page=3]{writeup-plots/ising_all-mcmc-runs}
  \end{center}
  \caption{ Results from MCMC estimates of the magnetization parameter $\beta$ in the Ising model.
  Vertical line shows the true value.
  }
\end{figure}

%%%
\subsection{CpG mutation, on a tree}

Nucleotide sequences of length $10^6$ were simulated down two (unequal) branches of a tree,
with all single-base substitution rates $m_{xy}$ set equal (varying from a total of .01 to .1 across the tree)
and an additional CpG rate $\gamma$ set to either 0 or three times the single-base rate.
Analysis was done with $\ell=2$ and $w=1$ (a five-to-one window).

The state at the root of the tree was independently chosen bases with equal frequencies,
and these frequencies at the root were added as parameters to the model (with a Dirichlet prior).

Here is a short example, with all parameters equal to 1, with each branch of the tree having length $0.1$:
\begin{center}
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc}
T&G&G&A&G&A&G&G&T&C&T&G&C&C&A&T&A&G&A&C&G&G&G&C&G&C&C&G&C&C&T&A&G&G&A&G&C&C&G&C&G&T&T&T&T&G&C&G&T&G&G&T&G&T&T&G&T&G&G&C \\
$\centerdot$&$\centerdot$&g&C&G&a&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&T&A&C&A&$\centerdot$&$\centerdot$&$\centerdot$&T&A&T&$\centerdot$&$\centerdot$&$\centerdot$&T&G&c&C&C&A&C&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&G&G&G&t&A&T&$\centerdot$&$\centerdot$&G&A&g&C&G&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&t&T&G&A \\
$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&A&T&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&T&T&G&T&$\centerdot$&$\centerdot$&G&A&T&$\centerdot$&$\centerdot$&$\centerdot$&G&G&c&t&A&A&G&C&$\centerdot$&$\centerdot$&$\centerdot$&G&T&G&A&t&t&t&g&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&G&A&$\centerdot$&$\centerdot$&$\centerdot$&$\centerdot$&A&C
\end{tabular}
\end{center}
Top is ancestral sequence (``unobserved''); bottom two are at the tips.

The point estimates are good,
as are the MCMC estimates,
but there seems to be mixing trouble having to do with the root frequencies.

\begin{figure}
  \begin{center}
    \includegraphics{writeup-plots/selsims-2013-06-03-13-17-0790276-initfreqs}
  \end{center}
  \caption{
  MCMC traces across 660,000 iterations of the frequencies at the root of the (two-taxon) tree in the CpG model.
  Vertical line shows the true value.
  }
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics{writeup-plots/selsims-2013-06-03-13-17-0790276-mutrates}
  \end{center}
  \caption{
  MCMC traces across 660,000 iterations of several mutation rate parameters in the CpG model.
  Units are in mean number of substitutions per base across the entire tree.
  Vertical line shows the true value.
  }
\end{figure}

%%%%%%%%
\section{Power sims}

Assess power and accuracy in the Ising model, by running replicates at these parameters:

\begin{tabular}{|rlllrrr|}
  \hline
  seqlen   &  $\lambda t$   &  $\beta$  &  $\gamma$  &  $w$ &  $\ell$   & \# reps \\
  \hline
  $10^4$    &   .2  &   1   &   .5  &   3   &   3   &   10  \\
  $10^3$    &   .2  &   1   &   .5  &   3   &   3   &   10  \\
  $10^2$    &   .2  &   1   &   .5  &   3   &   3   &   10  \\
  \hline
  $10^4$    &   .4  &   1   &   .5  &   3   &   3   &   10  \\
  $10^4$    &   .1  &   1   &   .5  &   3   &   3   &   10  \\
  $10^4$    &   .05  &   1   &   .5  &   3   &   3   &   10  \\
  \hline
  $10^4$    &   .2  &   1   &   0  &   3   &   3   &   10  \\
  \hline
  $10^4$    &   .2  &   1   &   .5  &   3   &   2   &   10  \\
  $10^4$    &   .2  &   1   &   .5  &   3   &   1   &   10  \\
  $10^4$    &   .2  &   1   &   .5  &   3   &   0   &   10  \\
  \hline
\end{tabular}


%%%%%%%%%%%%%%%%%%%%
\section{Discussion}


\begin{itemize}

  \item estimating freq spectrum [Hernandez]

  \item probabilistic ancestral states for common ancestor of e.g. humans 4N generations ago [Siepel] -- would widely used for primates

  \item ancestral CpG

  \item ancient DNA

  \item noisy image transmission denoising: first learn model; then reduce redundancy

  \item root distribution and confounding/mixing

  \item shrinkage prior on highly parameterized model

  \item Show that taking no context ($\ell=0$) can be positively misleading in some context.

  \item How to identify motifs in the residuals?

\end{itemize}

\subsection{Acknowledgements}
Erick, Jessica, Graham, Yaniv, Sergey, Simon, Matt, Rasmus

\bibliography{context-dependence}

\appendix

\section{Supplementary figures}

\begin{figure}
    \begin{center} 
        \includegraphics{writeup-plots/coverage_results_all}
    \end{center} 
    \caption{
        As in \ref{fig:ising_coverage}:
        Credible intervals for the four parameters of the dynamic Ising model,
        from MCMC runs on independently simulated sequences of $10^6$ bases
        differing at around 18.6\% of the sites.
        Shown are relative intervals, obtained by dividing by the true value.
        The three groups are left to right, credible intervals obtained from $(4,2,1)$, $(5,3,1)$, and $(6,2,2)$ T-mers:
        note that only the longest $(6,2,2)$ T-mers provide well-calibrated coverage (Table \ref{tab:ising_coverage})
        and do not show bias.
        \label{fig:all_ising_coverage}}
\end{figure}


%%%%
\section{Proof of the approximation}
\label{ss:approx_pf}

\begin{proof}

First we quantify and prove the approximation \eqref{eqn:window_approx}.
In fact, the approximation improves exponentially as $\ell$ and $r$ increase --
here is a simple argument.
Suppose the maximum length of any transition pattern is $R = \max\{ |u| : (\mu,u,v) \in \calT \}$.
The transition rate at site $i$ at time $s$ is determined if we know $X_{i-R}^{(2R+1)}(s)$;
this can be determined from $X_{i-R}^{2R+1}(0)$ and the outcome of all changes happening before time $s$ at sites in $(i-R-1, \ldots, i+R)$.
These will depend in turn on the outcome of previous possible changes.
For each site $i$ and times $s_1 < s_2$, define the set of states $H(i,s_1,s_2)$
so that if one knows the possible change points $s_{ij}$,
then changes outside of $H(i,s_1,s_2)$ do not affect the distribution of $X_i(s_2)$.
Concretely, define $H(i,s_1,s_2)$ by the property that $i \in H(i,s_1,s_2)$, and
\begin{gather}
  \text{if } s_1 < s_{jk} \le s_2 \text{ for some } i-R-1 \le j \le i+R, \text{ then } H(j,s_1,s_{jk}) \subseteq H(i,s_1,s_2) .
\end{gather}
In other words, $H(i,s_1,s_2)$ is the union of the $R$-neighborhoods of $i$ and all possible changes occurring in $H(i,s_1,s_2)$ between $s_1$ and $s_2$.
For a set of states $I$, let $H(I,s_1,s_2) = \bigcup_{i\in I} H(i,s_1,s_2)$.
Formally, the state $X_i(s_2)$ is conditionally independent of $\{X_j(s_1) \st j \notin H(i,s_1,s_2)\}$.
Informally, the approximation \eqref{eqn:window_approx} will be good if $H$ does note expand beyond the flanking context.
Now,
\begin{multline}
  \P\{ X_i^{(m)}(t) = y \mid X(0) \}
  = \P\left\{ H(\{i,\ldots,i+m\},0,t)\subseteq\{i-\ell,\ldots,i+m+r\} \right\} \\
   \qquad \qquad {} \times \P\left\{ X_i^{(m)}(t) = y \mid X_{i-\ell}^{(\ell+m+r)}(0), \; H(\{i,\ldots,i+m\},0,t)\subseteq\{i-\ell,\ldots,i+m+r\} \right\} \\
     \qquad {} + \P\left\{ H(\{i,\ldots,i+m\},0,t)\nsubseteq\{i-\ell,\ldots,i+m+r\} \right\}  \\
   \qquad\qquad {} \times \P\left\{ X_i^{(m)}(t) = y \mid X_{i-\ell}^{(\ell+m+r)}(0), \; H(\{i,\ldots,i+m\},0,t)\nsubseteq\{i-\ell,\ldots,i+m+r\} \right\} ,
\end{multline}
and so
\begin{align} \label{eqn:prob_approx}
  \left| \P\{ X_i^{(m)}(t) = y \mid X(0) \} - p_{\ell,m}(t;x,y) \right| \le 2 \P\left\{  H(\{I,\ldots,i+m\},0,t)\subseteq\{i-\ell,\ldots,i+m+r\} \right\}.
\end{align}

So, the error is bounded by the probability that $H(I,0,t)$ is larger than the context from $I-\ell$ to $I+r$.
However, following $H$ backwards in time, the outer edge of $H$ expands as a continuous-time random walk:
for instance, the left edge $L(s) = \min \{ j \in H(I,t-s,t) \}$ jumps leftwards at rate $R\mu_*$ with a random jump size
that has mean $R/2$.
General facts about random walks imply that \eqref{eqn:prob_approx} is bounded above by $C e^{-\alpha \ell/t}$, for some constants $C$ and $\alpha$.

\paragraph{Full likelihood} 
    Now we use these ideas to quantify and prove approximation \eqref{eqn:full_approx}.
For two positions $(i,s)$ and $(j,t)$ in sequence $\times$ time,
let $r(i,s;j,t) = |j-i|/|t-s|$ be the slope of the line between them,
and if $I$ and $J$ are subsets of sites, let $r(I,s;J,t) = \min\{r(i,s;j,t) : i\in I, j \in J\}$
be the minimum.
Above we have shown that there is a speed of dependency propagation $r_*$
such that if $r(I,s;J,t) > r_*$,
then $X_I(s)$ and $X_J(t)$ are nearly independent.
Now, fix $t$ and for each site $i$ let $N_i$ denote its $r_*$-neighborhood,
i.e.,
\begin{align}
    N_i = \{ (j,s) : r(i,t; j,s) \le r_*, \; 0 \le s \le t \} .
\end{align}
These are depicted in figure~\ref{fig:neighborhoods}A.
We will write $i \sim j$ if $|i-j| \le 2 r_* t$,
or equivalently, if $N_i \cap N_j \neq \emptyset$,
and $i \nsim j$ otherwise.
We can think of $N_i$ as the parts of ancestral sequences in which a mutation might have affected the state at $(i,t)$.
so if $i \nsim j$, then $i$ and $j$ are (approximately) independent at time $t$, given the sequence at time 0.
Concretely,
we have a small $\epsilon$ such that for any $i \nsim j$,
\begin{align}
    &\big|
    \log( \P\{ X_i(t) = x_i \and X_j(t) = x_j \} ) \\
    &\qquad {} -
    \log( \P\{ X_i(t) = x_i \} \P\{ X_j(t) = x_j \} )
    ) \big|
    \le \epsilon .
\end{align}


\begin{figure}
    \begin{center}
        \includegraphics{context-neighborhoods-defns}

        \vspace{2em}

        \includegraphics{context-neighborhoods-widths}
    \end{center}
    \caption{
        Diagram of things in the text:
        (top) Dependency neighboorhods.
        (bottom) What we need to compute $\P\{X_i(t)=x \given X(0), X_{<i}(t)\}$:
        site $i$ depends on the blue squares in $N_i$;
        the sites to the left of this in $L(i)$ are those other sites in $X(t)$
        whose dependency neighborhoods overlap $N_i$;
        any further sites to the left of this are basically independent of $X_i(t)$;
        the red squares give the dependency neighborhood of $L(i)$,
        so we need to condition on the row of red and blue sites at the top.
        On the bottom, we have $2w+1$ sites; at the top we have $4w+1$;
        here $w=2$.
        \label{fig:neighborhoods}
    }
\end{figure}


What we'd like to know is the likelihood, i.e.,
\begin{align}
    \like(y|x)
    &:=
    \P\{ X(t) = y \given X(0) = x \} .
\end{align}
If we define $X_{<i}=(X_1, \ldots, X_{i-1})$, then
\begin{align}
    \like(y|x)
    &=
    \prod_{i=1}^n
    \P\{ X_i(t) = y_i \given X(0) = x \and X_{<i}(t) = y_{<i} \} .
\end{align}
Now, defining $L(i) = \{ j < i \st j \sim i \}$
to be the sites to the left of $i$ whose dependency neighborhoods overlap with $i$'s,
and $A(I) = \{ j \st |j-i| \le r_*t \text{ for some } i \in I\}$
to be the ancestral sites in the dependency neighborhood of $I$,
\begin{align}
&\big|
\log( \P\{ X_i(t) = y_i \given X(0) = x \and X_{<i}(t) = y_{<i} \} ) \\
& \qquad {} -
\log( \P\{ X_i(t) = y_i \given X_{A(L(i))}(0) = x_{A(L(i))} \and X_{L(i)}(t) = y_{L(i)} \} )
 \big| \\
&\qquad \qquad \le
    \epsilon ,
\end{align}
so that if we define
\begin{align}
    \alike(y|x)
    &=
    \prod_{i=1}^n
    \P\{ X_i(t) = y_i \given X_{A(L(i))}(0) = x_{A(L(i))} \and X_{L(i)}(t) = y_{L(i)} \} ,
\end{align}
then (with $\loglike = \log \like$),
\begin{align}
    \left| \aloglike(y|x) - \loglike(y|x) \right|
    \le n \epsilon .
\end{align}

This is good, because we can compute $\aloglike$:
let $w=r_*t$ be radius of a dependency neighborhood,
so that the width of $L(i)$ is $2w$,
and for a sequence $x$ define $x^{(m)}_i = (x_{i-m+1}, \ldots, x_i)$:
\begin{align}
    \alike(y|x)
    &=
    \prod_{i=1}^n
    \frac{
        p_{w,2w+1}(y^{(2w+1)}_i|x^{(4w+1)}_{i+w})
    }{
        p_{w,2w}(y^{(2w)}_{i-1}|x^{(4w)}_{i-1+w})
    }
\end{align}
This is the ratio of the likelihoods of all the $(w,2w+1)$ T-mers,
divided by the likelihoods of all the $(w,2w)$ T-mers.


\end{proof}


\section{Peeling algorithm}
\label{ss:peeling_algorithm}

Fix one tip of the tree, $a$, to be the taxa where we count ``long'' sequences, let $\rho$ be the root, and let $b_1, \ldots, b_n$ be the remaining tips,
where we observe ``short'' sequences.
Let $v_0=\rho, v_1, \ldots, v_\ell = a$ be the path from the root down to $a$,
let $u_1, u_2, \ldots, u_m$ be the remaining internal nodes,
and for each $v_i$ let $u(v_i)$ be the offspring of $v_i$ that is not one of the other $v$.
For each node in the tree,
we will compute a matrix of probabilities:
for nodes not on the path from $a$ to the root,
we compute the chance that that each ``long'' sequence observed at that node
is matched by each possible combination of ``short'' sequences on the tips of the tree below it.
For nodes on the path from $a$ to the root, we compute the probability of observing
each ``long'' sequence at that node, along with the combinations of ``short'' sequences
on all tips further away from $a$ (i.e.\ below that node if it was rooted at $a$).
For a node $w$ this will be stored as a $|\calS|^{m+2\ell} \times |\calS|^{cm}$ matrix, for some $c$,
and denoted $M(w)$.
Rows of $M(u_j)$ sum to 1,
while each entire matrix $M(v_i)$ sums to 1;
the difference being that $M(v_i)$ sums over the prior at the root,
and $M(u_j)$ takes the value at $u_j$ as given.

Let $t(w)$ be the length of the branch above node $w$, for $w \neq \rho$,
and denote by $G^T$ the transpose of $G$.

\begin{enumerate}

  \item Set $M(b_i) = P$ for each $b_i$.

  \item While there are $u_j$ whose children $w_1$ and $w_2$ both have matrices computed,
    \begin{enumerate}
      \item let
        \begin{align}
          M(u_j)_{x,(y,z)} = \left( e^{t(w_1) G} M(w_1) \right)_{x,y} \left( e^{t(w_2) G} M(w_2)_{x,z} \right)_{x,z} .
        \end{align}
    \end{enumerate}

  \item Let
    \begin{align}
      M(\rho)_{x,y} = \pi(x) \left( e^{t(u(\rho)) G} M(w(\rho)) \right)_{x,y}
    \end{align}

  \item For each $1 \le k < \ell$,
    \begin{enumerate}
      \item let
        \begin{align}
          M(v_k)_{x,(y,z)} = \left( e^{t(v_k) G^T} M(v_{k-1}) \right)_{y,x} \left( e^{t(u(v_k)) G} M(u(v_{k-1})) \right)_{x,z}  .
        \end{align}
    \end{enumerate}

  \item Finally, let
    \begin{align}
      M(a)_{x,y} = \left( e^{t(a) G^T} M(v_{\ell-1}) \right)_{x,y}  .
    \end{align}

\end{enumerate}


%%%%
\section{Other likelihood functions}

\subsection{Summary statistics}

Since we cannot compute reasonably the full likelihood for the data,
we will need to choose appropriate summary statistics
that contain as much information about the process as possible,
and have a tractable expression for their marginal likelihood.
For simplicity,
we assume in this section that approximation \eqref{eqn:window_approx} holds exactly.
If $i_1 < \ldots < i_{n(x)}$ are a set of nonoverlapping sites where the initial sequence matches $x$
-- i.e.\ $X_{i_k}^{(2\ell+m)}(0)=x$ and $i_{k+1} > i_k + 2\ell+m$ for all $k$ %En dash close to math could be confusing. This sentence is a doozy.
then the counts $N(x,y)=\#\{ i_k \st X_{i_k+\ell}^{(m)}(t)=y\}$ (the number of these sites that match the smaller pattern $y$ at time $t$)
are multinomial, with probabilities $\{p_{\ell,m}(x,y)\st y \in \calS^m\}$.
For instance, if $\ell=3$ then $N(\text{ACTCAC}, \text{CGC})$ counts the number of occurrences of
\begin{center}
\begin{tabular}{c|ccccccc}
 $x$ &  C  & A & C & T & C & A & C \\
 $y$ &  $\centerdot$  & $\centerdot$  & C & G & C & $\centerdot$  & $\centerdot$
\end{tabular}
\end{center}
where $\cdot$ represents any base.
More generally, if we partition the sequence into windows of length $2\ell+m$,
and let $N(x,y)$ be the number of times we see a window in which $x$ matches $X(0)$ and $y$ matches $X(t)$,
then these counts are independently multinomial across each $x$.
% so the marginal likelihood of the statistics $N(x,y)$ is
% \begin{align}
%     \prod_{x \in \calS^{2\ell+m}} N(x)! \prod_{y \in \calS^m} \frac{p_{\ell,m}(x,y)^{N(x,y)}}{N(x,y)!} ,
% \end{align}
% where $N(x) = \sum_y N(x,y)$.

Of course, a drawback to this approach is that we throw away a good fraction of the data.
In fact, we do not even look at a fraction $2\ell/(2\ell+m)$ of the second sequence, $X(t)$,
that lies in the outside edges of each window.
As before, let $N(x)$ be the number of occurrences of a pattern $x$ in $X(0)$,
and $N(x,y)$ the number of times $x$ is seen juxtaposed with $y$ in $X(t)$,
but this time, count over all positions,
so that e.g.\ $N(x) = \#\{ 1 \le i \le n - 2\ell - m + 1 \st X_i^{(m+2\ell)}(0)=x\}$.

Now we will now calculate the expectation and covariances of $N(x,y)$.
\peter{The following is Not Right, but also we don't really use it at the moment.}
If we assume that \eqref{eqn:window_approx} is exact, then
\[
    \E [N(x,y)] = N(x) \, p_{\ell,m}(x,y).
\]
To compute covariances, define $\chi_i(x,y)$ to be 1 if $X_{i-\ell}^{(|x|)}(0)=x$ and $X_i^{(|y|)}(t)=y$, and 0 otherwise,
so that $N(x,y) = \sum_{i=1}^n \chi_i(x,y)$.
Overloading notation, also define $\chi_i(x)$ to be 1 if $X_{i-\ell}^{(|x|)}(0)=x$,
\begin{align}
  \var[N(x,y)] &= \sum_{i=1}^n \var[\chi_i(x,y)] + \sum_{i\neq j} \cov[\chi_i(x,y), \chi_j(x,y)] \\
               &\approx \sum_{i=1}^n \chi_i(x) p_{\ell,m}(x,y)(1-p_{\ell,m}(x,y))
            + 2 \sum_{i=1}^{n-2\ell-m} \sum_{j=i+1}^{i+2\ell+m-1} \cov[ \chi_i(x,y) \chi_j(x,y) ] \\
            &= N(x) p_{\ell,m}(x,y)(1-p_{\ell,m}(x,y)) +
\end{align}
The term $\E[\chi_1(x,y) \chi_{k+1}(x,y)]$ is the probability that $x$ matches at both site 1 and site $k+1$,
and that $y$ matches both at site $\ell+1$ and at site $\ell+k+1$.
For this to be nonzero, the joint pattern $(x,y)$ must allow self-overlaps.
Similarly, if $(x_1,y_1) \neq (x_2,y_2)$ then
\begin{align}
  \begin{split}
    \cov[N(x_1,y_1),N(x_2,y_2)]
    & \approx - n (2m+4\ell) p_{\ell,m}(x_1,y_1)p_{\ell,m}(x_2,y_2) \\
    & \qquad {} + n\sum_{k=1}^{m+2\ell} \E[\chi_1(x_1,y_1) \chi_{k+1}(x_2,y_2) + \chi_1(x_2,y_2) \chi_{k+1}(x_1,y_1)] .
  \end{split}
\end{align}


%%%%
\subsection{A likelihood function}

The covariance calculations show that
if $p_{\ell,m}(x,y)$ is small enough (i.e.\ the pattern is not common),
then $N(x,y)$ is close to Poisson.
Using this observation, we can estimate parameters by composite likelihood,
using all $N(x,y)$ as data.
Specifically, this composite likelihood is
\begin{align} \label{eqn:comp_like}
  \prod_{x,y} F_{(x,y)}^{N(x,y)}
\end{align}
where $F$ is the $|\calS|^{m+2\ell} \times |\calS|^m$ matrix whose $(x,y)^\text{th}$ entry is $p_{m,\ell}(t,x,y)$ as described in the "Computation" section.
Maximum likelihood then gets an asymptotically unbiased estimate of the parameters. (cite)
However, since they are not independent, to get confidence intervals, we need to either simulate, or do something different.

If we use only a subset of patterns $\{(x_k,y_k)\}$ chosen so that $\E[M_1(x,y) M_{k+1}(x,y)]$ is small (or zero),
\erick{M not defined, but I'm guessing that this is effectively saying that the probability of having two different patterns apply to the same sequences is small?}
the entire collection $\{N(x_k,y_k)\}$ will be close to independent Poisson.
(using Stein's method)
Then we'll have the correct likelihood function for (this subset of) data.

If changes are rare, then one way to get a set of patterns with this property is to only choose ones such that going from $x$ to $y$ entails a change;
and any overlapping patterns must require additional changes,
i.e.\ if $(x_1,y_1)$ matches at $i_1$,
then for $(x_2,y_2)$ to match at $i_2$,
there must be additional changes outside $(i_1,\ldots,i_1+\ell-1)$.

Here is one way to obtain such a set.
Take a pair of patterns $(x,y)$, that overlap at $m$ positions,
and suppose that they differ at $1 \le i_1, \ldots, i_d \le m$.
Let $\bar i = (1/d) \sum_{j=1}^d i_j$ be the mean position of the changes,
and take only patterns with $(m-1)/2 < \bar i \le (m+1)/2$.
For example, in the example above with $m=3$ and $\ell=2$,
\begin{center}
\begin{tabular}{c|ccccccc}
 $x$ &  C  & A & C & T & C & A & C \\
 $y$ &  $\centerdot$  & $\centerdot$  & C & G & C & $\centerdot$  & $\centerdot$
\end{tabular}
\end{center}
$x$ and $y$ differ only at (relative) position $i_1=2$.

Another way to achieve the same goal of having all overlapping patterns require additional changes is to require $i_1 = \bar i = 1$.
No two such patterns can be shifted versions of each other
without adding more changed sites,
since shifting by $k$ without adding changed sites just adds $k$ to $\bar i$.
For example, we might take
\begin{center}
\begin{tabular}{c|ccccccc}
 $x$ &  A & C & T & C & A & C & G \\
 $y$ &  $\centerdot$  & $\centerdot$  & G & C & A & $\centerdot$  & $\centerdot$
\end{tabular}
\end{center}

\section{Removed bits that we probably don't want}

\subsection{s5f likelihood}

The s5f model is defined as follows:
for each $(a,b,c,d,e,x) \subset \{\nA,\nC,\nG,\nT\}$,
it gives a (relative) mutation rate
\begin{align}
    \gamma(c,x;a,b,d,e) = \text{( mutation rate of $c \to x$ in context $abcde$ )} .
\end{align}
Given these rates, the only unknown parameter is the total amount of time
that the sequence has been mutated for.
Suppose that $X(t)$ is a sequence of length $n$ evolving under this model,
and let $G_n$ be the corresponding generator matrix,
a $4^n \times 4^n$ matrix.
(In the s5f model, the mutation rates are only specified in the middle of 5-mers,
so the two bases on each end never mutate.)
The transition probabilities are $e^{tG_n}$,
and by marginalization,
we can compute quantities like
the probability for observing the middle set of basis
given the flanking $\ell$ on each end:
\begin{align}
    p_{\ell,n-2\ell}(t;a,b)
    &=
    \P\{
        (X_{\ell+1}(t), X_{\ell+2}(t), \ldots, X_{n-\ell}(t))=(b_1,b_2,\ldots,b_{n-2\ell}) \\
    &\qquad \qquad
        \given (X_{1}(0), X_{2}(0), \ldots, X_{n}(0))=(a_1,a_2,\ldots,a_{n})
    \} .
\end{align}
If the size of the context, $\ell$, is big enough,
then this probability will not depend significantly if the sequence $a$ is in fact embedded in a longer sequence
(the error decreases exponentially with $\ell$).

The sequence $X(t)$ can be constructed by beginning with $X(0)$,
putting down a Poisson process of potential mutations of rate $\gamma = \max\{\gamma(c,x;a,b,d,e)\}$,
and thinning appropriately.
The process depends on the flanking 2bp,
so any other mutation within 2bp of a site affects the process at that site,
and the chance that this occurs depends on the flanking 4bp on either side.
Concretely, the number of flanking positions in each direction
that must be kept track of to decide what happens at a given site
grows by 1 at rate $\gamma$, and by 2 at rate $\gamma$;
so the amount of flanking sequence on each side affecting a given site at time $t$
is the sum of a Poisson($\gamma t$) and twice an independent Poisson($\gamma t$).
This has mean $3\gamma t$ and variance $5 \gamma t$;
with high probability it will not exceed, say,
$3\gamma t + 6 \sqrt{\gamma t}$;
if $\gamma t < 1$ this is less than 9.



%%%%%%%%
\subsection{Edge effects and cyclicization}

The choice to treat long sequences as cyclical is generally of little importance.
However, we have defined in \eqref{eqn:G_defn} the basic building block of our approximation, $G(m)$,
to be the transition matrix \emph{for a circular sequence} of length $m$, where $m$ may be short.
\peter{no we haven't}
This seems strange -- but, what else would we do at the edges?
We want to use $G(\ell+m+r)$ to compute the chance that a given $(\ell+m+r)$-sequence ends up with another given $m$-sequence in the middle;
and if $\ell$ and $r$ are big enough relative to $t$, this will be a good approximation regardless of what we do at the edges.
One answer would be to not allow patterns hanging off of either end to match,
resulting in a slight underestimate of the chance of change.
A better approximation would be to use the marginal frequencies in some way to average over surroudning sequence
-- but, this requires marginal frequencies to be known, and these are not stationary.
However, the leftmost site in a randomly chosen window is a sample from the marginal distribution,
and if sites separated by distance $\ell+m+r$ are sufficiently uncorrelated,
wrapping the sequence around is a good approximation to averaging over marginal frequencies in the surrounding sequence.
It is easy to imagine situations where this would not be the best solution --
but again, if $\ell$ and $r$ are large enough, it won't matter much.

%%%%
\subsection{The root distribution}

We clearly don't want to add an extra parameter $\pi(z)$ for all $z \in \calS^{m+2\ell}$.
If nothing else, it would make the inference depend on $m$ and $\ell$, which we'd like to avoid.
One approach would be to put a Dirichlet distribution on $\pi(z)$.
This has the nice property that you can choose priors that are consistent across different values of $m$ and $\ell$.
We'd have to still MCMC around in the space of $\pi(z)$, so they'd be nuisance parameters from that point of view.
Also, making the frequencies at the root random introduces correlations between counts, which we want to avoid.
An alternative is to specify $\pi$ through lower-dimensional statistics,
for instance to model the root state as a sample from a Markov random field
with parameters for the marginal frequencies of each state, and pairwise (or higher) correlations between them.

A Markov random field with interactions between sites no more than distance two away
is determined by the energy function $\phi(a,b,c)$ which gives the ``energy'' of the configuration $abc$ for each $a,b,c \in \calS$.
We use this to define the distribution on root patterns $\pi$ by
\begin{align}  \label{eqn:root_mrf}
    \pi(z) = \frac{1}{Z} \exp\left( - \sum_{i=1}^{|z|} \phi(z_{i-1},z_i,z_{i+1}) \right),
\end{align}
i.e.\ the exponential of the total energy of each three adjacent sites,
where $Z$ is the normalizing constant.
Note that it is often difficult to compute $Z$ in applications of Markov random fields,
but here we are working with relatively short sequences,
so we just compute \eqref{eqn:root_mrf} for all possible $z$, and divide by the sum.
(However, for this same reason this is not equivalent to saying that $X(0)$ is a sample from the Markov random field
and finding the distribution of patterns within this.)


%%%%%
\subsubsection{Mixing}

There can be significant confounding of the frequencies at the root with the other parameters,
which results in poor mixing.
(See CpG results, below.)
To see why this is, consider the following back-of-the-envelope calculation.
Take $x \neq y$ to be single bases (so, set $\ell=0$ and $w=1$),
so that $N(x,y)$ gives the number of times an $x$ and a $y$ are homologous.
Let $q(x,y)$ be the instantaneous rate that $x \to y$ (averaged somehow over context?),
so that $N(x,y)$ should be, roughly,
\begin{align*}
  \sum_z \pi(z) q(z,x) q(z,y) &\approx t \pi(x) (1+q(x,x)) q(x,y) + t \pi(y) q(y,x) (1+q(y,y)) ,
\end{align*}
where as usual $q(x,x) = (-1) \sum_{z \neq x} q(x,z)$ is (minus) the total jump rate out of $x$.

So, when we propose a change to $\pi$ we might want to also adjust the rates accordingly to keep this more-or-less constant:
if we chnage $\pi(x) \mapsto \pi(x) + \epsilon_x$, we want $\delta_{xy}$ to satisfy
\begin{align*}
  \sum_z \pi(z) q(z,x) q(z,y) = \sum_z (\pi(z) + \epsilon_z) ( q(z,x) + \delta_{zx} ) ( q(z,y) + \delta_{zy} ) ,
\end{align*}
or, to first order in $\delta$,
\begin{align*}
  0 &\approx \sum_z \epsilon_z q(z,x) q(z,y) + \sum_z \pi(z) \delta_{zx} q(z,y) + \sum_z \pi(z) q(z,x) \delta_{zy} \\
   &\approx \sum_z \epsilon_z q(z,x) q(z,y) + \sum_z (\pi(z)+\epsilon_z) \delta_{zx} q(z,y) + \sum_z (\pi(z)+\epsilon_z) q(z,x) \delta_{zy} \\
   &\approx \epsilon_x (1+q(x,x)) q(x,y) + \epsilon_y q(y,x) (1+q(y,y)) + (\pi(y)+\epsilon_y) \delta_{yx} (1+q(y,y)) + (\pi(x)+\epsilon_x) (1+q(x,x)) \delta_{xy} \\
   &= \left( \epsilon_x q(x,y) + (\pi(x)+\epsilon_x) \delta_{xy} \right)(1+q(x,x)) + \left( \epsilon_y q(y,x) + (\pi(y)+\epsilon_y) \delta_{yx} \right)(1+q(y,y))
\end{align*}
One solution to this is to let $\delta_{xy} = - \epsilon_x q(x,y) / \pi(x)$.

Suppose each change is a single-base change, so that changes $\delta \mu_i$ in $\mu$ contribute to $q$ by
$\delta_{xy} = \sum_{i: x \to y} \delta \mu_i$.
Given $\delta_{xy}$, this gives a linear equation for $\delta \mu_i$ that may or may not be solvable,
but certainly is if we have all the single-base transitions as paramters.

\subsection{Models without using potential}

Here is a reformulation of the CpG and Ising models without using a potential.

\begin{example}[CpG mutation]
    \peter{version without selection}
  A sequence of genome can be written using A, C, G, and T;
  in the most general model of independent mutation across sites, each of the 12 possible transitions occurs at its own rate.
  Furthermore, it is a well-known observation that in many species,
  adjacent, methylated CG dinucleotides (``CpG sites'') have a much higher mutation rate to TG and CA
  than either single nucleotide change under the independent model.
  Embellishing the single-nucleotide model with this additional rate results in the model defined by

  \begin{center}
    \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }cc}
      $x$  &  $y$  &  $m_{xy}$ & when $x \neq y \in \{\nA,\nC,\nG,\nT\}$  \\
      \nC\nG   &  \nT\nG   &  $\gamma$ & \\
      \nC\nG   &  \nC\nA   &  $\gamma$ &
    \end{tabular} .
  \end{center}
  Here $\gamma$ is the additional CpG rate above the base mutation rate,
  and a total of 13 parameters.
  Note that this model is very different in spirit to TASEP,
  in which the total number of particles was conserved.

  In this model, for a given pair of sequences, there may be more than one way to mutate a given string $x$ to get $y$:
  for instance, a change $\nA\nC\nG \to \nA\nT\nG$ could have occurred by a single $\nC \to \nT$ mutation,
  or by a $\nC\nG \to \nT\nG$ mutation;
  and hence the total instantaneous rate at which ACG changes to ATG is $\gamma + m_{\nC\nT}$.
  Although this means there are many possible parameterizations,
  note that the model is identifiable, not overparameterized.

\end{example}


\begin{example}[1-D Ising model with Glauber dynamics]
    \peter{version without selection}
  In the Ising model, each site is labeled as either ``up'' or ``down'' ($+1$ or $-1$ respectively),
  imagined as magnetic dipoles,
  and the energy associated with a given state $x$ is $H(x) = - \frac{1}{2} \beta \sum_i x_i x_{i+1} - \frac{1}{2} \gamma \sum_i x_i$.
  Here $\beta$ represents inverse temperature, and $\gamma$ represents the strength of the magnetic field (here scaled by temperature).
  The associated stationary distribution on configurations is proportional to $\exp(-H(x))$.
  The following ``Glauber dynamics'' preserve the stationary distribution:
  each site, independently at rate $\lambda$,
  forgets its spin,
  and reconfigures to a state chosen with probability proportional to the stationary probability of the resulting configuration.
  Ignoring transitions that don't change the state,
  if patterns $u$ and $v$ differ at only one site then $u$ changes to $v$ at rate $\lambda/(1+\exp(H(v)-H(u)))$,
  so

    \peter{reformulate with a potential}

  \begin{center}
    \begin{tabular}{c@{\quad$\to$\quad}c@{\quad at rate\quad }c}
      $+++$  &   $+-+$   &  $\lambda/\,(1+e^{2\beta + \gamma})$ \\
      $++-$  &   $+--$   &  $\lambda/(1+e^{\gamma})$ \\
      $+-+$  &   $+++$   &  $\lambda/(1+e^{-2\beta - \gamma})$ \\
      $+--$  &   $++-$   &  $\lambda/(1+e^{-\gamma})$ \\
      $-++$  &   $--+$   &  $\lambda/(1+e^{\gamma})$ \\
      $-+-$  &   $---$   &  $\lambda/(1+e^{-2\beta + \gamma})$ \\
      $--+$  &   $-++$   &  $\lambda/(1+e^{-\gamma})$ \\
      $---$  &   $-+-$   &  $\lambda/(1+e^{2\beta - \gamma})$
    \end{tabular}
  \end{center}


\end{example}


\end{document}
