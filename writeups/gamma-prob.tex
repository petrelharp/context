\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\given}{\mid}
\newcommand{\Beta}{B}
\newcommand{\deq}{\stackrel{\scriptscriptstyle{d}}{=}}

\begin{document}

Suppose that $(X_t)_{t\ge0}$ is a continuous-time Markov process with generator matrix $G$,
i.e.\ $G_{ij}>0$ if $i\neq j$ and rows of $G$ sum to zero.
Let $\tau$ be an independent Gamma distributed time with shape parameter $\alpha$ and scale parameter $\beta$.
We would like to compute the probabilities
\[
  Q(x,y;\alpha,\beta) := \P\{ X_\tau = y \given X_0 = x \} 
\]
in an efficient manner,
focusing on the case when $G$ is large and sparse.

Consider the corresponding ``jump chain'',
which represents $X$ as $X_t = M_{N(\lambda t)}$,
where $N(t)$ is a unit Poisson process and $M$ is a discrete-time Markov chain.
Here $\lambda$ is the maximum transition rate $\lambda = \max_i \{ - G_{ii} \}$,
and the transition matrix for $M$ is given by
\begin{align}
    P_{xy} &:= \P\{M_{k+1} = y \given M_k = x\} \\
        &= \begin{cases}  
                G_{xy}/\lambda \qquad & x\neq y \\
                1+G_{xx}/\lambda \qquad & x=y 
            \end{cases} .
\end{align}

$N(\lambda \tau)$ has a negative binomial distribution, with parameters $\alpha$ and $\lambda/(\lambda+\beta)$:
\begin{align}
  \P\{ N(\lambda \tau) = n \} &= \int_0^\infty \frac{ \beta^\alpha }{\Gamma(\alpha)} t^{\alpha-1} e^{-\beta t} e^{-\lambda t} \frac{ (\lambda t)^n }{ n! } dt \\
  &= \frac{ \beta^\alpha }{ \Gamma(\alpha) n! } \frac{\lambda^n}{(\lambda+\beta)^{n+\alpha}} \int_0^\infty t^{n+\alpha-1} e^{-t} dt \\
  &= \frac{ \Gamma(n+\alpha) }{ \Gamma(\alpha) \Gamma(n+1) } \frac{ \beta^\alpha \lambda^n }{(\lambda+\beta)^{n+\alpha}}\qquad \text{for } n \ge 0 .
\end{align}
It follows that
\begin{align}
    Q(x,y;\alpha,\beta) = \sum_{n \ge 0} \P\{ N(\lambda \tau) = n \} P^n  .
\end{align}
Note that 
\begin{align}
  \frac{ \P\{ N(\lambda \tau) = n+1 \} }{ \P\{ N(\lambda \tau) = n \} } &= \frac{ \Gamma(n+\alpha+1) \Gamma(n+1) }{ \Gamma(n+\alpha) \Gamma(n+2) } \frac{\lambda}{\lambda+\beta} \\
  &= \frac{ n+\alpha }{ n+1 } \frac{\lambda}{\lambda+\beta} ,
\end{align}
which is helpful for computing this.
The error in approximating this by a finite sum up to $n_0$ is bounded by $\P\{N(\lambda \tau)>n_0\}$.

%%%%%%%%%%%
\section{More calculations}

Suppose that $T$ is Exponential($\beta$)
and $S$ is Exponential($\beta+u$),
and that $N(t)$ is Poisson($\lambda t$).
By Poisson thinning,
\begin{align}
    S \deq T + \sum_{k=0}^M T_k,
\end{align}
where $T_k$ are independent copies of $T$
and $M$ is Geometric($\beta/(\beta+u)$).
Therefore, 
\begin{align}
    N(S) \deq N(T) + N',
\end{align}
where $N'$ is NegativeBinomial($M$,$\lambda/(\lambda+\beta)$).

What is a Geometric mixture of Negative Binomials?
Well, if 
\begin{align}
    \P\{ M = m \} &= p (1-p)^m \\
    \text{and } \P\{ N' = n \given M=m \} &= \frac{ \Gamma(n+m) }{ \Gamma(m) n! } q^m (1-q)^n, 
\end{align}
then
\begin{align}
    \P\{ N' = 0 \}
        &= \sum_{m \ge 0} p (1-p)^m q^m \\
        &= \frac{p}{1-q(1-p)} , \text{ and } \\
    \P\{ N' = n \} 
        &= \sum_{m \ge 1} p (1-p)^m \frac{ \Gamma(n+m) }{ \Gamma(m) n!} q^m (1-q)^n \\
        &= \frac{p(1-q)^n}{n!} \sum_{m \ge 1} q^m(1-p)^m \frac{ \Gamma(n+m) }{ \Gamma(m) } \\
        &= \frac{p(1-q)^n}{n!} \frac{ n! q (1-p) }{ (1-q(1-p))^n } \\
    &= (1-\frac{p}{1-q(1-p)}) \frac{ pq }{ 1-q } \left( \frac{1-q}{1-q(1-p)} \right)^n .
\end{align}
In other words,
if we let $a = (1-p)(1-q)/(1-q(1-p))$ and $b=(1-q)/(1-q(1-p))$,
then
\begin{align}
    \P\{ N' = 0 \} &= 1-a \\
    \P\{ N' = n \} &= a (1-b) b^{n-1} ,
\end{align}
i.e., if $X$ be Bernoulli($a$) and $Y$ be Geometric($b$) then $N'\deq X+Y$.

We want to apply this with $p=u/(\beta+u)$ and $q=\beta/(\lambda+\beta)$,
which translates to
\begin{align}
    a &= \frac{ \lambda \beta }{ u \lambda + u \beta + \lambda \beta } \\
      &= \frac{ \lambda/\beta }{ (1+\lambda/\beta)(1+u/\beta) - 1 } \\
    b &= \frac{ \lambda ( \beta + u ) }{ \lambda ( \beta + u ) + \beta u } 
\end{align}


\end{document}
